Collecting experience...
Ep:  0 | Ep_r:  26.0 | Steps:  25 | Ep_Loss: 0.0000
Ep:  5 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  10 | Ep_r:  22.0 | Steps:  21 | Ep_Loss: 0.0000
Ep:  15 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.0000
Ep:  20 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  25 | Ep_r:  53.0 | Steps:  52 | Ep_Loss: 0.0000
Ep:  30 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  35 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.0000
Ep:  40 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.0000
Ep:  45 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 0.0000
Ep:  50 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  55 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  60 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.0000
Ep:  65 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.0000
Ep:  70 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 0.0000
Ep:  75 | Ep_r:  13.0 | Steps:  12 | Ep_Loss: 0.0000
Ep:  80 | Ep_r:  13.0 | Steps:  12 | Ep_Loss: 0.0000
Ep:  85 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 0.0000
Ep:  90 | Ep_r:  30.0 | Steps:  29 | Ep_Loss: 0.0000
Ep:  95 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 0.0000
Ep:  100 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.0000
Ep:  105 | Ep_r:  15.0 | Steps:  14 | Ep_Loss: 9.4662
Ep:  110 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 4.3162
Ep:  115 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 2.5218
Ep:  120 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 3.2061
Ep:  125 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 1.0831
Ep:  130 | Ep_r:  18.0 | Steps:  17 | Ep_Loss: 1.3534
Ep:  135 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 1.8184
Ep:  140 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 1.1622
Ep:  145 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 1.1294
/home/quantumiracle/research/ensemble-dqn/dqn.py:135: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  wrap_tensor = lambda x: torch.tensor([x])
Ep:  150 | Ep_r:  15.0 | Steps:  14 | Ep_Loss: 1.1054
Ep:  155 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 2.0870
Ep:  160 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 1.2430
Ep:  165 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.9555
Ep:  170 | Ep_r:  65.0 | Steps:  64 | Ep_Loss: 3.3787
Ep:  175 | Ep_r:  29.0 | Steps:  28 | Ep_Loss: 1.2162
Ep:  180 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 0.6494
Ep:  185 | Ep_r:  23.0 | Steps:  22 | Ep_Loss: 0.8207
Ep:  190 | Ep_r:  27.0 | Steps:  26 | Ep_Loss: 1.1742
Ep:  195 | Ep_r:  23.0 | Steps:  22 | Ep_Loss: 0.9802
Ep:  200 | Ep_r:  38.0 | Steps:  37 | Ep_Loss: 1.4563
Ep:  205 | Ep_r:  45.0 | Steps:  44 | Ep_Loss: 1.5812
Ep:  210 | Ep_r:  111.0 | Steps:  110 | Ep_Loss: 3.5881
Ep:  215 | Ep_r:  13.0 | Steps:  12 | Ep_Loss: 0.3459
Ep:  220 | Ep_r:  38.0 | Steps:  37 | Ep_Loss: 1.1060
Ep:  225 | Ep_r:  186.0 | Steps:  185 | Ep_Loss: 5.2810
Ep:  230 | Ep_r:  94.0 | Steps:  93 | Ep_Loss: 2.7654
Ep:  235 | Ep_r:  40.0 | Steps:  39 | Ep_Loss: 1.0927
Ep:  240 | Ep_r:  150.0 | Steps:  149 | Ep_Loss: 3.9123
Ep:  245 | Ep_r:  112.0 | Steps:  111 | Ep_Loss: 2.4990
Ep:  250 | Ep_r:  119.0 | Steps:  118 | Ep_Loss: 3.0339
Ep:  255 | Ep_r:  22.0 | Steps:  21 | Ep_Loss: 0.5471
Ep:  260 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 0.3361
Ep:  265 | Ep_r:  125.0 | Steps:  124 | Ep_Loss: 2.6322
Ep:  270 | Ep_r:  113.0 | Steps:  112 | Ep_Loss: 2.3737
Ep:  275 | Ep_r:  116.0 | Steps:  115 | Ep_Loss: 2.6725
Ep:  280 | Ep_r:  47.0 | Steps:  46 | Ep_Loss: 0.8798
Ep:  285 | Ep_r:  18.0 | Steps:  17 | Ep_Loss: 0.2758
Ep:  290 | Ep_r:  137.0 | Steps:  136 | Ep_Loss: 2.1780
Ep:  295 | Ep_r:  111.0 | Steps:  110 | Ep_Loss: 1.5535
Ep:  300 | Ep_r:  111.0 | Steps:  110 | Ep_Loss: 1.2332
Ep:  305 | Ep_r:  316.0 | Steps:  315 | Ep_Loss: 3.2013
Ep:  310 | Ep_r:  205.0 | Steps:  204 | Ep_Loss: 1.6375
Ep:  315 | Ep_r:  205.0 | Steps:  204 | Ep_Loss: 1.3987
Ep:  320 | Ep_r:  352.0 | Steps:  351 | Ep_Loss: 2.8616
Ep:  325 | Ep_r:  170.0 | Steps:  169 | Ep_Loss: 1.4944
Ep:  330 | Ep_r:  187.0 | Steps:  186 | Ep_Loss: 1.3651
Ep:  335 | Ep_r:  130.0 | Steps:  129 | Ep_Loss: 0.9257
Ep:  340 | Ep_r:  126.0 | Steps:  125 | Ep_Loss: 0.7274
Ep:  345 | Ep_r:  223.0 | Steps:  222 | Ep_Loss: 1.4341
Ep:  350 | Ep_r:  374.0 | Steps:  373 | Ep_Loss: 2.0361
Ep:  355 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.6863
Ep:  360 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.5249
Ep:  365 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.5541
Ep:  370 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.7462
Ep:  375 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.6483
Ep:  380 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.8035
Ep:  385 | Ep_r:  375.0 | Steps:  374 | Ep_Loss: 2.3151
Ep:  390 | Ep_r:  169.0 | Steps:  168 | Ep_Loss: 0.9590
Ep:  395 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.8663
Ep:  400 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.0016
Ep:  405 | Ep_r:  236.0 | Steps:  235 | Ep_Loss: 1.4440
Ep:  410 | Ep_r:  106.0 | Steps:  105 | Ep_Loss: 0.6610
Ep:  415 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.0507
Ep:  420 | Ep_r:  408.0 | Steps:  407 | Ep_Loss: 2.0766
Ep:  425 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.7487
Ep:  430 | Ep_r:  325.0 | Steps:  324 | Ep_Loss: 2.3266
Ep:  435 | Ep_r:  113.0 | Steps:  112 | Ep_Loss: 0.6900
Ep:  440 | Ep_r:  103.0 | Steps:  102 | Ep_Loss: 0.6266
Ep:  445 | Ep_r:  68.0 | Steps:  67 | Ep_Loss: 0.4150
Ep:  450 | Ep_r:  133.0 | Steps:  132 | Ep_Loss: 0.7922
Ep:  455 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.0102
Ep:  460 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.1202
Ep:  465 | Ep_r:  321.0 | Steps:  320 | Ep_Loss: 1.8328
Ep:  470 | Ep_r:  137.0 | Steps:  136 | Ep_Loss: 0.8421
Ep:  475 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.0387
Ep:  480 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.4242
Ep:  485 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.2463
Ep:  490 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.7514
Ep:  495 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.0726