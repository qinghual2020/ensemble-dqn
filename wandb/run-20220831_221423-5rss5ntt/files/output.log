Collecting experience...
Ep:  0 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 0.0000
Ep:  5 | Ep_r:  18.0 | Steps:  17 | Ep_Loss: 0.0000
Ep:  10 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.0000
Ep:  15 | Ep_r:  23.0 | Steps:  22 | Ep_Loss: 0.0000
Ep:  20 | Ep_r:  33.0 | Steps:  32 | Ep_Loss: 0.0000
Ep:  25 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  30 | Ep_r:  56.0 | Steps:  55 | Ep_Loss: 0.0000
Ep:  35 | Ep_r:  13.0 | Steps:  12 | Ep_Loss: 0.0000
Ep:  40 | Ep_r:  13.0 | Steps:  12 | Ep_Loss: 0.0000
Ep:  45 | Ep_r:  23.0 | Steps:  22 | Ep_Loss: 0.0000
Ep:  50 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.0000
Ep:  55 | Ep_r:  33.0 | Steps:  32 | Ep_Loss: 0.0000
Ep:  60 | Ep_r:  29.0 | Steps:  28 | Ep_Loss: 0.0000
Ep:  65 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 0.0000
Ep:  70 | Ep_r:  44.0 | Steps:  43 | Ep_Loss: 0.0000
Ep:  75 | Ep_r:  54.0 | Steps:  53 | Ep_Loss: 0.0000
Ep:  80 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.0000
Ep:  85 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 0.0000
Ep:  90 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.0000
Ep:  95 | Ep_r:  35.0 | Steps:  34 | Ep_Loss: 26.2543
Ep:  100 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 16.8828
Ep:  105 | Ep_r:  19.0 | Steps:  18 | Ep_Loss: 3.8978
Ep:  110 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 3.9757
Ep:  115 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 1.3050
Ep:  120 | Ep_r:  27.0 | Steps:  26 | Ep_Loss: 2.1859
Ep:  125 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.7652
/home/quantumiracle/research/ensemble-dqn/dqn.py:135: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  wrap_tensor = lambda x: torch.tensor([x])
Ep:  130 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 1.5755
Ep:  135 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 1.5221
Ep:  140 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 1.5247
Ep:  145 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 1.4769
Ep:  150 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 1.2126
Ep:  155 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.7992
Ep:  160 | Ep_r:  66.0 | Steps:  65 | Ep_Loss: 2.8462
Ep:  165 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.4278
Ep:  170 | Ep_r:  28.0 | Steps:  27 | Ep_Loss: 1.1045
Ep:  175 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.4026
Ep:  180 | Ep_r:  27.0 | Steps:  26 | Ep_Loss: 1.0013
Ep:  185 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.5165
Ep:  190 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 0.7416
Ep:  195 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.4843
Ep:  200 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 0.6531
Ep:  205 | Ep_r:  61.0 | Steps:  60 | Ep_Loss: 1.9582
Ep:  210 | Ep_r:  80.0 | Steps:  79 | Ep_Loss: 2.3172
Ep:  215 | Ep_r:  64.0 | Steps:  63 | Ep_Loss: 1.9019
Ep:  220 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 0.4832
Ep:  225 | Ep_r:  143.0 | Steps:  142 | Ep_Loss: 4.2240
Ep:  230 | Ep_r:  40.0 | Steps:  39 | Ep_Loss: 1.0235
Ep:  235 | Ep_r:  98.0 | Steps:  97 | Ep_Loss: 2.3256
Ep:  240 | Ep_r:  40.0 | Steps:  39 | Ep_Loss: 0.8234
Ep:  245 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.3742
Ep:  250 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 0.7157
Ep:  255 | Ep_r:  22.0 | Steps:  21 | Ep_Loss: 0.5961
Ep:  260 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 0.7167
Ep:  265 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.4141
Ep:  270 | Ep_r:  18.0 | Steps:  17 | Ep_Loss: 0.3565
Ep:  275 | Ep_r:  107.0 | Steps:  106 | Ep_Loss: 2.3106
Ep:  280 | Ep_r:  19.0 | Steps:  18 | Ep_Loss: 0.3144
Ep:  285 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.2845
Ep:  290 | Ep_r:  90.0 | Steps:  89 | Ep_Loss: 1.7348
Ep:  295 | Ep_r:  77.0 | Steps:  76 | Ep_Loss: 1.3915
Ep:  300 | Ep_r:  74.0 | Steps:  73 | Ep_Loss: 1.0634
Ep:  305 | Ep_r:  76.0 | Steps:  75 | Ep_Loss: 0.7318
Ep:  310 | Ep_r:  74.0 | Steps:  73 | Ep_Loss: 0.7046
Ep:  315 | Ep_r:  193.0 | Steps:  192 | Ep_Loss: 1.8375
Ep:  320 | Ep_r:  184.0 | Steps:  183 | Ep_Loss: 1.3257
Ep:  325 | Ep_r:  118.0 | Steps:  117 | Ep_Loss: 0.8575
Ep:  330 | Ep_r:  85.0 | Steps:  84 | Ep_Loss: 0.5606
Ep:  335 | Ep_r:  106.0 | Steps:  105 | Ep_Loss: 0.7060
Ep:  340 | Ep_r:  210.0 | Steps:  209 | Ep_Loss: 1.5239
Ep:  345 | Ep_r:  135.0 | Steps:  134 | Ep_Loss: 0.8856
Ep:  350 | Ep_r:  90.0 | Steps:  89 | Ep_Loss: 0.5412
Ep:  355 | Ep_r:  217.0 | Steps:  216 | Ep_Loss: 1.2553
Ep:  360 | Ep_r:  121.0 | Steps:  120 | Ep_Loss: 0.6851
Ep:  365 | Ep_r:  265.0 | Steps:  264 | Ep_Loss: 1.4301
Ep:  370 | Ep_r:  218.0 | Steps:  217 | Ep_Loss: 1.1485
Ep:  375 | Ep_r:  273.0 | Steps:  272 | Ep_Loss: 1.4886
Ep:  380 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.6932
Ep:  385 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.4879
Ep:  390 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.6560
Ep:  395 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 2.7724
Ep:  400 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.0896
Ep:  405 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.6611
Ep:  410 | Ep_r:  189.0 | Steps:  188 | Ep_Loss: 1.3798
Ep:  415 | Ep_r:  282.0 | Steps:  281 | Ep_Loss: 2.1551
Ep:  420 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.2996
Ep:  425 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.6565
Ep:  430 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 4.7192
Ep:  435 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.8345
Ep:  440 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.3644
Ep:  445 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.5197
Ep:  450 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.4546
Ep:  455 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 3.5547
Ep:  460 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 4.0285
Ep:  465 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 4.2340
Ep:  470 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 4.1800
Ep:  475 | Ep_r:  103.0 | Steps:  102 | Ep_Loss: 1.5576
Ep:  480 | Ep_r:  64.0 | Steps:  63 | Ep_Loss: 0.6509
Ep:  485 | Ep_r:  56.0 | Steps:  55 | Ep_Loss: 0.7259
Ep:  490 | Ep_r:  58.0 | Steps:  57 | Ep_Loss: 0.4291
Ep:  495 | Ep_r:  369.0 | Steps:  368 | Ep_Loss: 3.3687
Collecting experience...
Ep:  0 | Ep_r:  30.0 | Steps:  29 | Ep_Loss: 0.0000
Ep:  5 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.0000
Ep:
Ep:  35 | Ep_r:  45.0 | Steps:  44 | Ep_Loss: 0.0000
Ep:  15 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  20 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 0.0000
Ep:  25 | Ep_r:  15.0 | Steps:  14 | Ep_Loss: 0.0000
Ep:  30 | Ep_r:  13.0 | Steps:  12 | Ep_Loss: 0.0000
Ep:  35 | Ep_r:  45.0 | Steps:  44 | Ep_Loss: 0.0000
Ep:  40 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  45 | Ep_r:  37.0 | Steps:  36 | Ep_Loss: 0.0000
Ep:  50 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.0000
Ep:  55 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.0000
Ep:  60 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  65 | Ep_r:  13.0 | Steps:  12 | Ep_Loss: 0.0000
Ep:  70 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.0000
Ep:  75 | Ep_r:  23.0 | Steps:  22 | Ep_Loss: 0.0000
Ep:  80 | Ep_r:  44.0 | Steps:  43 | Ep_Loss: 0.0000
Ep:  85 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.0000
Ep:  90 | Ep_r:  18.0 | Steps:  17 | Ep_Loss: 0.1478
Ep:  95 | Ep_r:  22.0 | Steps:  21 | Ep_Loss: 2.0768
Ep:  100 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 1.1447
Ep:  105 | Ep_r:  68.0 | Steps:  67 | Ep_Loss: 5.4192
Ep:  110 | Ep_r:  41.0 | Steps:  40 | Ep_Loss: 3.4033
Ep:  115 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.8615
Ep:  120 | Ep_r:  18.0 | Steps:  17 | Ep_Loss: 0.8630
Ep:  125 | Ep_r:  23.0 | Steps:  22 | Ep_Loss: 0.4505
Ep:  130 | Ep_r:  49.0 | Steps:  48 | Ep_Loss: 1.0063
Ep:  135 | Ep_r:  31.0 | Steps:  30 | Ep_Loss: 0.6011
Ep:  140 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 0.3422
Ep:  145 | Ep_r:  49.0 | Steps:  48 | Ep_Loss: 0.8227
Ep:  150 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 0.3213
Ep:  155 | Ep_r:  78.0 | Steps:  77 | Ep_Loss: 1.0039
Ep:  160 | Ep_r:  19.0 | Steps:  18 | Ep_Loss: 0.1963
Ep:  165 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.1064
Ep:  170 | Ep_r:  51.0 | Steps:  50 | Ep_Loss: 0.6302
Ep:  175 | Ep_r:  68.0 | Steps:  67 | Ep_Loss: 0.6593
Ep:  180 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.1266
Ep:  185 | Ep_r:  39.0 | Steps:  38 | Ep_Loss: 0.3581
Ep:  190 | Ep_r:  69.0 | Steps:  68 | Ep_Loss: 0.6339
Ep:  195 | Ep_r:  90.0 | Steps:  89 | Ep_Loss: 0.8658
Ep:  200 | Ep_r:  26.0 | Steps:  25 | Ep_Loss: 0.2624
Ep:  205 | Ep_r:  52.0 | Steps:  51 | Ep_Loss: 0.6987
Ep:  210 | Ep_r:  72.0 | Steps:  71 | Ep_Loss: 1.7915
Ep:  215 | Ep_r:  22.0 | Steps:  21 | Ep_Loss: 0.8916
Ep:  220 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.2871
Ep:  225 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.3841
Ep:  230 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.2359
Ep:  235 | Ep_r:  25.0 | Steps:  24 | Ep_Loss: 0.4402
Ep:  240 | Ep_r:  119.0 | Steps:  118 | Ep_Loss: 1.6830
Ep:  245 | Ep_r:  68.0 | Steps:  67 | Ep_Loss: 0.9299
Ep:  250 | Ep_r:  29.0 | Steps:  28 | Ep_Loss: 0.3423
Ep:  255 | Ep_r:  197.0 | Steps:  196 | Ep_Loss: 1.9236
Ep:  260 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 4.3608
Ep:  265 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 5.0138
Ep:  270 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 4.8826
Ep:  275 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 5.6858

Ep:  280 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 5.6994
Ep:  285 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 5.2694
Ep:  290 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 5.7508
Ep:  295 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.4075
Ep:  300 | Ep_r:  15.0 | Steps:  14 | Ep_Loss: 0.7630
Ep:  305 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.4724
Ep:  310 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.4053
Ep:  315 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.3479
Ep:  320 | Ep_r:  8.0 | Steps:  7 | Ep_Loss: 0.2433
Ep:  325 | Ep_r:  8.0 | Steps:  7 | Ep_Loss: 0.2859
Ep:  330 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.4606
Ep:  335 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.4054
Ep:  340 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.3485
Ep:  345 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.3467
Ep:  350 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.5496
Ep:  355 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 6.0326
Ep:  360 | Ep_r:  178.0 | Steps:  177 | Ep_Loss: 2.0264
Ep:  365 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 6.9206
Ep:  370 | Ep_r:  197.0 | Steps:  196 | Ep_Loss: 3.1668
Ep:  375 | Ep_r:  122.0 | Steps:  121 | Ep_Loss: 1.8319
Ep:  380 | Ep_r:  89.0 | Steps:  88 | Ep_Loss: 1.5239
Ep:  385 | Ep_r:  77.0 | Steps:  76 | Ep_Loss: 1.0117
Ep:  390 | Ep_r:  89.0 | Steps:  88 | Ep_Loss: 1.3054
Ep:  395 | Ep_r:  74.0 | Steps:  73 | Ep_Loss: 1.3596
Ep:  400 | Ep_r:  71.0 | Steps:  70 | Ep_Loss: 1.0414
Ep:  405 | Ep_r:  84.0 | Steps:  83 | Ep_Loss: 1.5646
Ep:  410 | Ep_r:  80.0 | Steps:  79 | Ep_Loss: 1.3952
Ep:  415 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.7214
Ep:  420 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.4454
Ep:  425 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.6718
Ep:  430 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.7031
Ep:  435 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.6558
Ep:  440 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.2487
Ep:  445 | Ep_r:  8.0 | Steps:  7 | Ep_Loss: 0.1955
Ep:  450 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.4954
Ep:  455 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.4610
Ep:  460 | Ep_r:  8.0 | Steps:  7 | Ep_Loss: 0.4090
Ep:  465 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.3600
Ep:  470 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.4765
Ep:  475 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.4819
Ep:  480 | Ep_r:  9.0 | Steps:  8 | Ep_Loss: 0.3219
Ep:  485 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.4323
Ep:  490 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 8.2802
Ep:  495 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 5.4321
[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
Collecting experience...
Ep:  0 | Ep_r:  34.0 | Steps:  33 | Ep_Loss: 0.0000
Ep:  5 | Ep_r:  28.0 | Steps:  27 | Ep_Loss: 0.0000
Ep:  10 | Ep_r:  28.0 | Steps:  27 | Ep_Loss: 0.0000
Ep:  15 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.0000
Ep:  20 | Ep_r:  55.0 | Steps:  54 | Ep_Loss: 0.0000
Ep:  25 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 0.0000
Ep:  30 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  35 | Ep_r:  13.0 | Steps:  12 | Ep_Loss: 0.0000
Ep:  40 | Ep_r:  27.0 | Steps:  26 | Ep_Loss: 0.0000
Ep:  45 | Ep_r:  46.0 | Steps:  45 | Ep_Loss: 0.0000
Ep:  50 | Ep_r:  29.0 | Steps:  28 | Ep_Loss: 0.0000
Ep:  55 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.0000
Ep:  60 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 0.0000
Ep:  65 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 0.0000
Ep:  70 | Ep_r:  27.0 | Steps:  26 | Ep_Loss: 0.0000
Ep:  75 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.0000
Ep:  80 | Ep_r:  40.0 | Steps:  39 | Ep_Loss: 0.0000
Ep:  85 | Ep_r:  23.0 | Steps:  22 | Ep_Loss: 0.0000
Ep:  90 | Ep_r:  25.0 | Steps:  24 | Ep_Loss: 0.3755
Ep:  95 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.1196
Ep:  100 | Ep_r:  19.0 | Steps:  18 | Ep_Loss: 0.2058
Ep:  105 | Ep_r:  62.0 | Steps:  61 | Ep_Loss: 0.7115
Ep:  110 | Ep_r:  96.0 | Steps:  95 | Ep_Loss: 1.0088
Ep:  115 | Ep_r:  63.0 | Steps:  62 | Ep_Loss: 0.6192
Ep:  120 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.1016
Ep:  125 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.1893
Ep:  130 | Ep_r:  29.0 | Steps:  28 | Ep_Loss: 0.2686
Ep:  135 | Ep_r:  53.0 | Steps:  52 | Ep_Loss: 0.4546
Ep:  140 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 0.2031
Ep:  145 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.1696
Ep:  150 | Ep_r:  38.0 | Steps:  37 | Ep_Loss: 0.3285
Ep:  155 | Ep_r:  30.0 | Steps:  29 | Ep_Loss: 0.3289
Ep:  160 | Ep_r:  11.0 | Steps:  10 | Ep_Loss: 0.0981
Ep:  165 | Ep_r:  29.0 | Steps:  28 | Ep_Loss: 0.2414
Ep:  170 | Ep_r:  76.0 | Steps:  75 | Ep_Loss: 0.7774
Ep:  175 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 0.3329
Ep:  180 | Ep_r:  32.0 | Steps:  31 | Ep_Loss: 0.3496
Ep:  185 | Ep_r:  28.0 | Steps:  27 | Ep_Loss: 0.3270
Ep:  190 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 0.2717
Ep:  195 | Ep_r:  26.0 | Steps:  25 | Ep_Loss: 0.2875
Ep:  200 | Ep_r:  34.0 | Steps:  33 | Ep_Loss: 0.4652
Ep:  205 | Ep_r:  64.0 | Steps:  63 | Ep_Loss: 0.8265
Ep:  210 | Ep_r:  50.0 | Steps:  49 | Ep_Loss: 0.6199
Ep:  215 | Ep_r:  25.0 | Steps:  24 | Ep_Loss: 0.4022
Ep:  220 | Ep_r:  184.0 | Steps:  183 | Ep_Loss: 2.2092
Ep:  225 | Ep_r:  56.0 | Steps:  55 | Ep_Loss: 0.5226
Ep:  230 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.1140
Ep:  235 | Ep_r:  77.0 | Steps:  76 | Ep_Loss: 0.8063
Ep:  240 | Ep_r:  89.0 | Steps:  88 | Ep_Loss: 0.8592
Ep:  245 | Ep_r:  21.0 | Steps:  20 | Ep_Loss: 0.1693
Ep:  250 | Ep_r:  135.0 | Steps:  134 | Ep_Loss: 1.2273
Ep:  255 | Ep_r:  23.0 | Steps:  22 | Ep_Loss: 0.1803
Ep:  260 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 0.1988
Ep:  265 | Ep_r:  55.0 | Steps:  54 | Ep_Loss: 0.5630
Ep:  270 | Ep_r:  197.0 | Steps:  196 | Ep_Loss: 1.9497
Ep:  275 | Ep_r:  181.0 | Steps:  180 | Ep_Loss: 1.6276
Ep:  280 | Ep_r:  87.0 | Steps:  86 | Ep_Loss: 0.9293
Ep:  285 | Ep_r:  173.0 | Steps:  172 | Ep_Loss: 1.5942
Ep:  290 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 5.1860
Ep:  295 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 4.7614
Traceback (most recent call last):
  File "train_dqn.py", line 72, in <module>
    rollout(env, model, i, writer)
  File "train_dqn.py", line 43, in rollout
    loss = model.learn(sample)
  File "/home/quantumiracle/research/ensemble-dqn/dqn.py", line 205, in learn
    Q_s_prime_a_prime = (Q_s_prime_a_prime-Q_s_prime_a_prime.mean())/ (Q_s_prime_a_prime.std() + 1e-5)  # normalization
