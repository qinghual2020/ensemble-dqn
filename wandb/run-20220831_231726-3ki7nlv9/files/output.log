Collecting experience...
Ep:  0 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.0000
Ep:  5 | Ep_r:  27.0 | Steps:  26 | Ep_Loss: 0.0000
Ep:  10 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.0000
Ep:  15 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  20 | Ep_r:  10.0 | Steps:  9 | Ep_Loss: 0.0000
Ep:  25 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  30 | Ep_r:  36.0 | Steps:  35 | Ep_Loss: 0.0000
Ep:  35 | Ep_r:  46.0 | Steps:  45 | Ep_Loss: 0.0000
Ep:  40 | Ep_r:  17.0 | Steps:  16 | Ep_Loss: 0.0000
Ep:  45 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 0.0000
Ep:  50 | Ep_r:  15.0 | Steps:  14 | Ep_Loss: 0.0000
Ep:  55 | Ep_r:  15.0 | Steps:  14 | Ep_Loss: 0.0000
Ep:  60 | Ep_r:  14.0 | Steps:  13 | Ep_Loss: 0.0000
Ep:  65 | Ep_r:  22.0 | Steps:  21 | Ep_Loss: 0.0000
Ep:  70 | Ep_r:  19.0 | Steps:  18 | Ep_Loss: 0.0000
Ep:  75 | Ep_r:  12.0 | Steps:  11 | Ep_Loss: 0.0000
Ep:  80 | Ep_r:  18.0 | Steps:  17 | Ep_Loss: 0.0000
Ep:  85 | Ep_r:  53.0 | Steps:  52 | Ep_Loss: 0.0000
Ep:  90 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 0.0000
Ep:  95 | Ep_r:  20.0 | Steps:  19 | Ep_Loss: 10.2314
Ep:  100 | Ep_r:  21.0 | Steps:  20 | Ep_Loss: 14.8798
Ep:  105 | Ep_r:  24.0 | Steps:  23 | Ep_Loss: 12.1273
Ep:  110 | Ep_r:  15.0 | Steps:  14 | Ep_Loss: 2.6127
ensemble_dqn.py:136: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  wrap_tensor = lambda x: torch.tensor([x])
Ep:  115 | Ep_r:  48.0 | Steps:  47 | Ep_Loss: 5.7672
Ep:  120 | Ep_r:  16.0 | Steps:  15 | Ep_Loss: 1.6689
Ep:  125 | Ep_r:  71.0 | Steps:  70 | Ep_Loss: 5.2297
Ep:  130 | Ep_r:  18.0 | Steps:  17 | Ep_Loss: 1.0032
Ep:  135 | Ep_r:  27.0 | Steps:  26 | Ep_Loss: 1.4140
Ep:  140 | Ep_r:  72.0 | Steps:  71 | Ep_Loss: 2.9845
Ep:  145 | Ep_r:  39.0 | Steps:  38 | Ep_Loss: 1.6230
Ep:  150 | Ep_r:  113.0 | Steps:  112 | Ep_Loss: 4.5152
Ep:  155 | Ep_r:  52.0 | Steps:  51 | Ep_Loss: 2.2404
Ep:  160 | Ep_r:  81.0 | Steps:  80 | Ep_Loss: 3.2483
Ep:  165 | Ep_r:  31.0 | Steps:  30 | Ep_Loss: 1.1718
Ep:  170 | Ep_r:  129.0 | Steps:  128 | Ep_Loss: 4.7983
Ep:  175 | Ep_r:  52.0 | Steps:  51 | Ep_Loss: 2.0680
Ep:  180 | Ep_r:  267.0 | Steps:  266 | Ep_Loss: 12.1451
Ep:  185 | Ep_r:  145.0 | Steps:  144 | Ep_Loss: 7.4926
Ep:  190 | Ep_r:  21.0 | Steps:  20 | Ep_Loss: 1.0202
Ep:  195 | Ep_r:  177.0 | Steps:  176 | Ep_Loss: 7.0829
Ep:  200 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 21.7410
Ep:  205 | Ep_r:  92.0 | Steps:  91 | Ep_Loss: 4.1825
Ep:  210 | Ep_r:  375.0 | Steps:  374 | Ep_Loss: 17.8588
Ep:  215 | Ep_r:  128.0 | Steps:  127 | Ep_Loss: 6.7417
Ep:  220 | Ep_r:  308.0 | Steps:  307 | Ep_Loss: 13.9827
Ep:  225 | Ep_r:  435.0 | Steps:  434 | Ep_Loss: 16.9079
Ep:  230 | Ep_r:  441.0 | Steps:  440 | Ep_Loss: 15.5606
Ep:  235 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 17.0781
Ep:  240 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 15.8017
Ep:  245 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 17.3443
Ep:  250 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 18.0368
Ep:  255 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 18.2358
Ep:  260 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 19.4009
Ep:  265 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 19.7365
Ep:  270 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 19.8070
Ep:  275 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 20.3286
Ep:  280 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 22.0963
Ep:  285 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 21.7262
Ep:  290 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 22.0372
Ep:  295 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 22.8945

Ep:  300 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 21.9189
Ep:  305 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 22.2543
Ep:  310 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 23.9735
Ep:  315 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 23.5604
Ep:  320 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 24.2922
Ep:  325 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 24.0645
Ep:  330 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 25.0070
Ep:  335 | Ep_r:  285.0 | Steps:  284 | Ep_Loss: 15.2668
Ep:  340 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 25.1713
Ep:  345 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 25.5329
Ep:  350 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 26.7834
Ep:  355 | Ep_r:  351.0 | Steps:  350 | Ep_Loss: 17.3741
Ep:  360 | Ep_r:  322.0 | Steps:  321 | Ep_Loss: 15.1846
Ep:  365 | Ep_r:  295.0 | Steps:  294 | Ep_Loss: 13.9545
Ep:  370 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 24.5908
Ep:  375 | Ep_r:  118.0 | Steps:  117 | Ep_Loss: 5.3921
Ep:  380 | Ep_r:  118.0 | Steps:  117 | Ep_Loss: 6.2377
Ep:  385 | Ep_r:  350.0 | Steps:  349 | Ep_Loss: 17.0769
Ep:  390 | Ep_r:  477.0 | Steps:  476 | Ep_Loss: 25.4580
Ep:  395 | Ep_r:  150.0 | Steps:  149 | Ep_Loss: 7.9802
Ep:  400 | Ep_r:  152.0 | Steps:  151 | Ep_Loss: 9.0958
Ep:  405 | Ep_r:  455.0 | Steps:  454 | Ep_Loss: 23.7602
Ep:  410 | Ep_r:  218.0 | Steps:  217 | Ep_Loss: 10.3561
Ep:  415 | Ep_r:  432.0 | Steps:  431 | Ep_Loss: 23.6561
Ep:  420 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 25.7726
Ep:  425 | Ep_r:  368.0 | Steps:  367 | Ep_Loss: 19.4128
Ep:  430 | Ep_r:  173.0 | Steps:  172 | Ep_Loss: 9.1631
Ep:  435 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 25.9057
Ep:  440 | Ep_r:  234.0 | Steps:  233 | Ep_Loss: 13.1959
Ep:  445 | Ep_r:  116.0 | Steps:  115 | Ep_Loss: 6.7873
Ep:  450 | Ep_r:  123.0 | Steps:  122 | Ep_Loss: 7.3593
Ep:  455 | Ep_r:  155.0 | Steps:  154 | Ep_Loss: 8.9856
Ep:  460 | Ep_r:  316.0 | Steps:  315 | Ep_Loss: 18.8926
Ep:  465 | Ep_r:  209.0 | Steps:  208 | Ep_Loss: 13.2192
Ep:  470 | Ep_r:  186.0 | Steps:  185 | Ep_Loss: 11.5766
Ep:  475 | Ep_r:  206.0 | Steps:  205 | Ep_Loss: 13.0227
Ep:  480 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 32.9326
Ep:  485 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 31.3858
Ep:  490 | Ep_r:  124.0 | Steps:  123 | Ep_Loss: 8.2606
Ep:  495 | Ep_r:  500.0 | Steps:  499 | Ep_Loss: 37.4163